{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNLH4NkkTonS",
        "outputId": "9bc54824-7241-45aa-eb27-edda463fd19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.29.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
        "from torch.utils.data import Dataset\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import TrainingArguments, Trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEdU7bsCemCv",
        "outputId": "aef1e8ae-22a4-4831-abd0-0677583caf2e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up GPU\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jKRodKzsTwYW",
        "outputId": "457f045e-8cba-47d1-dcc4-d32c56acc634"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/UofT/IMDB_Dataset.csv' # @Team: Replace with your own google drive path to dataset\n",
        "df = pd.read_csv(file_path)\n",
        "df = df.sample(frac=1.0, random_state=413)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "01CtmeqZTyGL",
        "outputId": "8c5081b2-f83c-4da7-9dcc-c204b655bc67"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review sentiment\n",
              "21315  First of all, it is interesting to note that o...  positive\n",
              "20835  The unthinkable has happened. Having first wit...  negative\n",
              "29274  One of the best records of Israel's response t...  positive\n",
              "32234  But, lets face it... it got a few nostalgic si...  negative\n",
              "26597  Ben a out-of-town cop is convinced his sister ...  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f278159e-ff3d-4736-89a7-46cdfd931bde\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21315</th>\n",
              "      <td>First of all, it is interesting to note that o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20835</th>\n",
              "      <td>The unthinkable has happened. Having first wit...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29274</th>\n",
              "      <td>One of the best records of Israel's response t...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32234</th>\n",
              "      <td>But, lets face it... it got a few nostalgic si...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26597</th>\n",
              "      <td>Ben a out-of-town cop is convinced his sister ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f278159e-ff3d-4736-89a7-46cdfd931bde')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f278159e-ff3d-4736-89a7-46cdfd931bde button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f278159e-ff3d-4736-89a7-46cdfd931bde');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-32fe2b4a-5da3-45c2-bc6a-beb1c4c58dfd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-32fe2b4a-5da3-45c2-bc6a-beb1c4c58dfd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-32fe2b4a-5da3-45c2-bc6a-beb1c4c58dfd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"One of the best Tarzan films is also one of its most action packed (and graphic).<br /><br />Picking up a year or so after Tarzan the Ape Man, Niel Hamilton's Holt has asked a rich friend to finance a safari back to the elephants graveyard to collect ivory. His Friend arrives also carrying dresses and perfumes that Holt hopes to use to win Jane back from Tarzan. Before they can leave Holt finds his map stolen and it becomes a mad dash to try and capture a competing expedition. When they finally over take the thieves they find the whole party dead and themselves surrounded. They have no choice but to fight their way out and soon find they are out of the frying pan and into the fire. Eventually Tarzan and Jane show up and everyone is off on even more adventures.<br /><br />Infamous film was heavily censored to reduce the graphic violence (Its graphic even by todays standards. It probably would get a PG 13) and to remove all hint of nudity, (there is a several minute long nude swim scene involving Jane that is full frontal in its nudity, it was only recently restored). Its clear watching the restored version why this film was reduced by 20 minutes in its run time for TV. As it stands in its restored version this is a very adult film that is romantic, touching, action filled and everything else that a movie should be. Its an amazing film by almost any standard. Best of all its the sort of film that plays well both as a stand alone adventure, one need not to have seen the first film to enjoy it, but its also a film that deepens the characters and themes that were set up in that original film. Its an amazing thing.<br /><br />I really like this film a great deal.<br /><br />If there are any flaws to the film, its perhaps that the film hasn't aged well. The rear screen is often very obvious, there are gorilla suits for many of the apes and some of the other effects are more quaint rather than convincing. However on almost every other level this film is top notch.<br /><br />You really owe it to yourself to see this. Make yourself a big bowl of popcorn and curl up on the couch and just let yourself drift back to a simpler time. This is one of the great adventures.\",\n          \"While HOUSE OF WAX will never be mistaken for Texas CHAINSAW MASSACRE or HALLOWEEN, it does try and ultimately succeed in being a worthy addition to quality horror film making. There is enough tension and enough implied gore in here to please the young audience of today and there is also enough explicit gore to please some of the purists of yesterday. Overall, it is quite a well done film that should do okay on DVD.<br /><br />Six kids on the way to a football game get stranded in a DELIVERANCE like setting and what ensues actually looks like it may have been taken out a discarded Deilverance sequel and the backwoods hicks seem to taunt the group. Eventually the group has to break up and half of them head into the desolate town to get some car parts. The other half of the group stays behind at camp and this sets up the second half of the film.<br /><br />House of Wax has many strengths. The first is that it is blessed with a very likable cast. Paris Hilton is fine in her role but she is far from the focal point. Chad Michael Murray, Elisabeth Cuthbert and Brian Van Holt are three very charismatic actors and their presence in the film adds some panache, some credibility and some flair. I had never really heard of Chad Michael Murray, only that he was a pin-up type teen heartthrob. But he has a presence to him. He imbues an intangible to him that seems to translate into his performance. He is instantly likable and he possesses a strong character trait. I enjoyed his performance much more than I figured I would and when he was on screen, the film flowed.<br /><br />The question now becomes, \\\"Is this film a worthy horror film?\\\" And the answer to that is a resounding YES. As I mentioned off the top, it is not in the same class a NIGHTMARE ON ELM STREET or LAST HOUSE ON THE LEFT, or even THE RING, but it is relatively tense, quite graphic and there are some very inventive death scenes. This is not the HOUSE OF WAX that your parents told you about. Vincent Price is nowhere to be seen and when you start seeing fingers being cut off and hot wax covering comatose bodies, you realize this film is a little on the edge.<br /><br />I'd like to see an uncut director's X rated version. I bet there is one out there somewhere that would be much more violent than this one. Be that as it may, this is a very good entry into the horror genre and it is heads and shoulders above weak films like I KNOW WHAT YOU DID LAST SUMMER and DARK WATER and a few others. It is definitely worth a look.<br /><br />8/10\",\n          \"Dolph Lundgren stars as Murray Wilson an alcoholic ex-cop who gets involved with a serial killer who kills during sex, after his brother is murdered, Wilson starts his own investigation and finds out a lot of his brother's secrets in this very dull thriller. Lundgren mails in his performance and the movie is flat and lethargic. Also when has anyone watched a Dolph Lundgren movie for anything but action?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentiment to binary labels\n",
        "df.rename(columns={'sentiment': 'labels'}, inplace=True)\n",
        "label_mapping = {'positive': 1, 'negative': 0}\n",
        "df['labels'] = df['labels'].map(label_mapping)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5MGpK3VHT6E0",
        "outputId": "25c3820a-7d1a-4955-9e1e-a59605c524df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review  labels\n",
              "21315  First of all, it is interesting to note that o...       1\n",
              "20835  The unthinkable has happened. Having first wit...       0\n",
              "29274  One of the best records of Israel's response t...       1\n",
              "32234  But, lets face it... it got a few nostalgic si...       0\n",
              "26597  Ben a out-of-town cop is convinced his sister ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90687abf-32eb-49af-9290-abab7cdcf692\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21315</th>\n",
              "      <td>First of all, it is interesting to note that o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20835</th>\n",
              "      <td>The unthinkable has happened. Having first wit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29274</th>\n",
              "      <td>One of the best records of Israel's response t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32234</th>\n",
              "      <td>But, lets face it... it got a few nostalgic si...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26597</th>\n",
              "      <td>Ben a out-of-town cop is convinced his sister ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90687abf-32eb-49af-9290-abab7cdcf692')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-90687abf-32eb-49af-9290-abab7cdcf692 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-90687abf-32eb-49af-9290-abab7cdcf692');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-25aded9a-0767-49c7-afd1-df9d60241e4a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-25aded9a-0767-49c7-afd1-df9d60241e4a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-25aded9a-0767-49c7-afd1-df9d60241e4a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"One of the best Tarzan films is also one of its most action packed (and graphic).<br /><br />Picking up a year or so after Tarzan the Ape Man, Niel Hamilton's Holt has asked a rich friend to finance a safari back to the elephants graveyard to collect ivory. His Friend arrives also carrying dresses and perfumes that Holt hopes to use to win Jane back from Tarzan. Before they can leave Holt finds his map stolen and it becomes a mad dash to try and capture a competing expedition. When they finally over take the thieves they find the whole party dead and themselves surrounded. They have no choice but to fight their way out and soon find they are out of the frying pan and into the fire. Eventually Tarzan and Jane show up and everyone is off on even more adventures.<br /><br />Infamous film was heavily censored to reduce the graphic violence (Its graphic even by todays standards. It probably would get a PG 13) and to remove all hint of nudity, (there is a several minute long nude swim scene involving Jane that is full frontal in its nudity, it was only recently restored). Its clear watching the restored version why this film was reduced by 20 minutes in its run time for TV. As it stands in its restored version this is a very adult film that is romantic, touching, action filled and everything else that a movie should be. Its an amazing film by almost any standard. Best of all its the sort of film that plays well both as a stand alone adventure, one need not to have seen the first film to enjoy it, but its also a film that deepens the characters and themes that were set up in that original film. Its an amazing thing.<br /><br />I really like this film a great deal.<br /><br />If there are any flaws to the film, its perhaps that the film hasn't aged well. The rear screen is often very obvious, there are gorilla suits for many of the apes and some of the other effects are more quaint rather than convincing. However on almost every other level this film is top notch.<br /><br />You really owe it to yourself to see this. Make yourself a big bowl of popcorn and curl up on the couch and just let yourself drift back to a simpler time. This is one of the great adventures.\",\n          \"While HOUSE OF WAX will never be mistaken for Texas CHAINSAW MASSACRE or HALLOWEEN, it does try and ultimately succeed in being a worthy addition to quality horror film making. There is enough tension and enough implied gore in here to please the young audience of today and there is also enough explicit gore to please some of the purists of yesterday. Overall, it is quite a well done film that should do okay on DVD.<br /><br />Six kids on the way to a football game get stranded in a DELIVERANCE like setting and what ensues actually looks like it may have been taken out a discarded Deilverance sequel and the backwoods hicks seem to taunt the group. Eventually the group has to break up and half of them head into the desolate town to get some car parts. The other half of the group stays behind at camp and this sets up the second half of the film.<br /><br />House of Wax has many strengths. The first is that it is blessed with a very likable cast. Paris Hilton is fine in her role but she is far from the focal point. Chad Michael Murray, Elisabeth Cuthbert and Brian Van Holt are three very charismatic actors and their presence in the film adds some panache, some credibility and some flair. I had never really heard of Chad Michael Murray, only that he was a pin-up type teen heartthrob. But he has a presence to him. He imbues an intangible to him that seems to translate into his performance. He is instantly likable and he possesses a strong character trait. I enjoyed his performance much more than I figured I would and when he was on screen, the film flowed.<br /><br />The question now becomes, \\\"Is this film a worthy horror film?\\\" And the answer to that is a resounding YES. As I mentioned off the top, it is not in the same class a NIGHTMARE ON ELM STREET or LAST HOUSE ON THE LEFT, or even THE RING, but it is relatively tense, quite graphic and there are some very inventive death scenes. This is not the HOUSE OF WAX that your parents told you about. Vincent Price is nowhere to be seen and when you start seeing fingers being cut off and hot wax covering comatose bodies, you realize this film is a little on the edge.<br /><br />I'd like to see an uncut director's X rated version. I bet there is one out there somewhere that would be much more violent than this one. Be that as it may, this is a very good entry into the horror genre and it is heads and shoulders above weak films like I KNOW WHAT YOU DID LAST SUMMER and DARK WATER and a few others. It is definitely worth a look.<br /><br />8/10\",\n          \"Dolph Lundgren stars as Murray Wilson an alcoholic ex-cop who gets involved with a serial killer who kills during sex, after his brother is murdered, Wilson starts his own investigation and finds out a lot of his brother's secrets in this very dull thriller. Lundgren mails in his performance and the movie is flat and lethargic. Also when has anyone watched a Dolph Lundgren movie for anything but action?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean data\n",
        "def clean_data(text):\n",
        "    english_stopwords = set(stopwords.words(\"english\"))\n",
        "    cleaned_text = []\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)\n",
        "    text = re.sub(f'[{re.escape(punctuation)}]', ' ', text)\n",
        "    for token in text.split():\n",
        "        if token.lower() not in english_stopwords and not token.isdigit():\n",
        "            cleaned_text.append(token.lower())\n",
        "    return ' '.join(cleaned_text)\n",
        "\n",
        "# Apply clean_data function to 'review' column\n",
        "df['review'] = df['review'].apply(lambda x: clean_data(x))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hohBXXk5Xpn5",
        "outputId": "24ecfab9-966f-4d25-8af4-0aee4856fdc1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review  labels\n",
              "21315  first interesting note one users commented fil...       1\n",
              "20835  unthinkable happened first witnessed years ago...       0\n",
              "29274  one best records israel response murder rabin ...       1\n",
              "32234  lets face got nostalgic sighs show consistentl...       0\n",
              "26597  ben town cop convinced sister brutally killed ...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3908132f-0759-4f8e-b5c0-995943e3c66a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21315</th>\n",
              "      <td>first interesting note one users commented fil...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20835</th>\n",
              "      <td>unthinkable happened first witnessed years ago...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29274</th>\n",
              "      <td>one best records israel response murder rabin ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32234</th>\n",
              "      <td>lets face got nostalgic sighs show consistentl...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26597</th>\n",
              "      <td>ben town cop convinced sister brutally killed ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3908132f-0759-4f8e-b5c0-995943e3c66a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3908132f-0759-4f8e-b5c0-995943e3c66a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3908132f-0759-4f8e-b5c0-995943e3c66a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1b33325e-06b2-490d-be79-d32b8bb9b2b8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1b33325e-06b2-490d-be79-d32b8bb9b2b8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1b33325e-06b2-490d-be79-d32b8bb9b2b8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49575,\n        \"samples\": [\n          \"short terrible disappointment far worst adaptation one favourite novels dialogue horribly clumsy could sense feeling behind words expressed characters lines delivered hastily felt rather place could well cited statues chemistry george c scott susannah york non existent watching american rochester felt strange could least tried british accent like george c scott actor simply work felt like watching highlights jane eyre main pieces story randomly put together regard flow story scenery music nice could feel none passion love supposed jane rochester movie left totally unmoved want see good version jane eyre suggest watch bbc version timothy dalton zelah clarke version ciar\\u00e1n hinds samantha morton two brilliant adaptations\",\n          \"awesomely improbable foolish potboiler least redeeming crisp location photography unbelievable generate much way tension kinda hoping stanwyck make back time really saddled wet ways one husband idiot child well run meeker nagging question remains sort wood pier support made rotten piece pulled float stanwyck always impeccably professional best could material threadbare\",\n          \"matter whether drew leelee total babes lot girls pretty hot appear nerdy movie oscar type movie least good point view life like young people real people made us laugh learn accept others really movie represents real world really matters\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BERT Tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", max_length=1024)"
      ],
      "metadata": {
        "id": "7z-f34wcVraJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BERT model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rkxy4bVVyo3",
        "outputId": "1c35fddb-a327-44a4-a58e-5f63a69e12ee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into 70% training, 10% validation, 20% testing\n",
        "train_texts = list(df.review[:35000])\n",
        "val_texts = list(df.review[35000:40000])\n",
        "test_texts = list(df.review[40000:])\n",
        "\n",
        "train_labels = list(df.labels[:35000])\n",
        "val_labels = list(df.labels[35000:40000])\n",
        "test_labels = list(df.labels[40000:])"
      ],
      "metadata": {
        "id": "j_uBLCmUWUwT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize different datasets\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings  = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "IsJP8_tNXR5L"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "b1jM8zBAXWV_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate custom datasets\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)\n",
        "test_dataset = CustomDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "Ez-djQZEY_xQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute accuracy and metrics\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
      ],
      "metadata": {
        "id": "1wqDqQuVZgll"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments with lr = 1e-4\n",
        "training_args3 = TrainingArguments(\n",
        "    output_dir='./',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=20,\n",
        "    per_device_eval_batch_size=30,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=1e-4,\n",
        "    eval_steps=50,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Define trainer with arguments from above\n",
        "trainer3 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args3,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics= compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIvwABKcaF0j",
        "outputId": "f8ed6e78-79a8-4228-af6b-eca388474a7b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune pretrained Bert Model\n",
        "trainer3.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D9D6qmaUf0ml",
        "outputId": "ba8104cd-1ec7-430e-b079-3504b4952dfc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5250' max='5250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5250/5250 59:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.634700</td>\n",
              "      <td>0.373981</td>\n",
              "      <td>0.831000</td>\n",
              "      <td>0.833750</td>\n",
              "      <td>0.831145</td>\n",
              "      <td>0.830693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.491000</td>\n",
              "      <td>0.595497</td>\n",
              "      <td>0.766200</td>\n",
              "      <td>0.825237</td>\n",
              "      <td>0.766880</td>\n",
              "      <td>0.755392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.436800</td>\n",
              "      <td>0.342837</td>\n",
              "      <td>0.853800</td>\n",
              "      <td>0.859159</td>\n",
              "      <td>0.853605</td>\n",
              "      <td>0.853202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.447100</td>\n",
              "      <td>0.431354</td>\n",
              "      <td>0.845200</td>\n",
              "      <td>0.848525</td>\n",
              "      <td>0.845044</td>\n",
              "      <td>0.844787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.455600</td>\n",
              "      <td>0.413748</td>\n",
              "      <td>0.832000</td>\n",
              "      <td>0.832203</td>\n",
              "      <td>0.832039</td>\n",
              "      <td>0.831985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.405000</td>\n",
              "      <td>0.373090</td>\n",
              "      <td>0.858000</td>\n",
              "      <td>0.858092</td>\n",
              "      <td>0.857974</td>\n",
              "      <td>0.857983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.390300</td>\n",
              "      <td>0.376399</td>\n",
              "      <td>0.854800</td>\n",
              "      <td>0.860616</td>\n",
              "      <td>0.855003</td>\n",
              "      <td>0.854261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.464300</td>\n",
              "      <td>0.442300</td>\n",
              "      <td>0.824200</td>\n",
              "      <td>0.848745</td>\n",
              "      <td>0.823775</td>\n",
              "      <td>0.820917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.528200</td>\n",
              "      <td>0.679338</td>\n",
              "      <td>0.498400</td>\n",
              "      <td>0.249200</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.332621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.601100</td>\n",
              "      <td>0.425271</td>\n",
              "      <td>0.846000</td>\n",
              "      <td>0.849673</td>\n",
              "      <td>0.845836</td>\n",
              "      <td>0.845550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.523300</td>\n",
              "      <td>0.440466</td>\n",
              "      <td>0.827800</td>\n",
              "      <td>0.846666</td>\n",
              "      <td>0.827426</td>\n",
              "      <td>0.825310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.442800</td>\n",
              "      <td>0.375214</td>\n",
              "      <td>0.854600</td>\n",
              "      <td>0.856778</td>\n",
              "      <td>0.854475</td>\n",
              "      <td>0.854346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.413700</td>\n",
              "      <td>0.369776</td>\n",
              "      <td>0.835400</td>\n",
              "      <td>0.846901</td>\n",
              "      <td>0.835108</td>\n",
              "      <td>0.833939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.483600</td>\n",
              "      <td>0.400898</td>\n",
              "      <td>0.844400</td>\n",
              "      <td>0.848845</td>\n",
              "      <td>0.844219</td>\n",
              "      <td>0.843853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.383500</td>\n",
              "      <td>0.587042</td>\n",
              "      <td>0.766600</td>\n",
              "      <td>0.827614</td>\n",
              "      <td>0.765908</td>\n",
              "      <td>0.754887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.526200</td>\n",
              "      <td>0.460069</td>\n",
              "      <td>0.800400</td>\n",
              "      <td>0.833349</td>\n",
              "      <td>0.799896</td>\n",
              "      <td>0.795156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.440400</td>\n",
              "      <td>0.416037</td>\n",
              "      <td>0.832400</td>\n",
              "      <td>0.845242</td>\n",
              "      <td>0.832091</td>\n",
              "      <td>0.830734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.387600</td>\n",
              "      <td>0.466469</td>\n",
              "      <td>0.826200</td>\n",
              "      <td>0.851060</td>\n",
              "      <td>0.825774</td>\n",
              "      <td>0.822935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.405500</td>\n",
              "      <td>0.344715</td>\n",
              "      <td>0.868000</td>\n",
              "      <td>0.868449</td>\n",
              "      <td>0.868056</td>\n",
              "      <td>0.867971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.368800</td>\n",
              "      <td>0.346207</td>\n",
              "      <td>0.846200</td>\n",
              "      <td>0.862586</td>\n",
              "      <td>0.845860</td>\n",
              "      <td>0.844351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.395100</td>\n",
              "      <td>0.305272</td>\n",
              "      <td>0.871800</td>\n",
              "      <td>0.871895</td>\n",
              "      <td>0.871826</td>\n",
              "      <td>0.871796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.330100</td>\n",
              "      <td>0.341726</td>\n",
              "      <td>0.882600</td>\n",
              "      <td>0.884014</td>\n",
              "      <td>0.882503</td>\n",
              "      <td>0.882472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.354700</td>\n",
              "      <td>0.386263</td>\n",
              "      <td>0.876200</td>\n",
              "      <td>0.880580</td>\n",
              "      <td>0.876372</td>\n",
              "      <td>0.875877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.340200</td>\n",
              "      <td>0.337375</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.885411</td>\n",
              "      <td>0.884948</td>\n",
              "      <td>0.884958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.373900</td>\n",
              "      <td>0.383114</td>\n",
              "      <td>0.841400</td>\n",
              "      <td>0.864984</td>\n",
              "      <td>0.840993</td>\n",
              "      <td>0.838682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.347600</td>\n",
              "      <td>0.370633</td>\n",
              "      <td>0.871600</td>\n",
              "      <td>0.877127</td>\n",
              "      <td>0.871407</td>\n",
              "      <td>0.871085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.419900</td>\n",
              "      <td>0.339855</td>\n",
              "      <td>0.887400</td>\n",
              "      <td>0.887478</td>\n",
              "      <td>0.887378</td>\n",
              "      <td>0.887389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.401000</td>\n",
              "      <td>0.347800</td>\n",
              "      <td>0.877200</td>\n",
              "      <td>0.883275</td>\n",
              "      <td>0.876999</td>\n",
              "      <td>0.876669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.343600</td>\n",
              "      <td>0.379275</td>\n",
              "      <td>0.876200</td>\n",
              "      <td>0.883449</td>\n",
              "      <td>0.875980</td>\n",
              "      <td>0.875566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.416100</td>\n",
              "      <td>0.319951</td>\n",
              "      <td>0.887800</td>\n",
              "      <td>0.888101</td>\n",
              "      <td>0.887756</td>\n",
              "      <td>0.887769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.313273</td>\n",
              "      <td>0.875400</td>\n",
              "      <td>0.880654</td>\n",
              "      <td>0.875212</td>\n",
              "      <td>0.874928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.361200</td>\n",
              "      <td>0.295041</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.888223</td>\n",
              "      <td>0.884855</td>\n",
              "      <td>0.884732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.329000</td>\n",
              "      <td>0.351360</td>\n",
              "      <td>0.866200</td>\n",
              "      <td>0.875062</td>\n",
              "      <td>0.866446</td>\n",
              "      <td>0.865459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.330600</td>\n",
              "      <td>0.304924</td>\n",
              "      <td>0.892200</td>\n",
              "      <td>0.892205</td>\n",
              "      <td>0.892207</td>\n",
              "      <td>0.892200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.363900</td>\n",
              "      <td>0.332120</td>\n",
              "      <td>0.869800</td>\n",
              "      <td>0.876838</td>\n",
              "      <td>0.869582</td>\n",
              "      <td>0.869140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.247900</td>\n",
              "      <td>0.425425</td>\n",
              "      <td>0.885800</td>\n",
              "      <td>0.886752</td>\n",
              "      <td>0.885880</td>\n",
              "      <td>0.885744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.362300</td>\n",
              "      <td>0.358713</td>\n",
              "      <td>0.874800</td>\n",
              "      <td>0.880680</td>\n",
              "      <td>0.874602</td>\n",
              "      <td>0.874272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.308100</td>\n",
              "      <td>0.428712</td>\n",
              "      <td>0.852200</td>\n",
              "      <td>0.864073</td>\n",
              "      <td>0.851911</td>\n",
              "      <td>0.850911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.370500</td>\n",
              "      <td>0.375369</td>\n",
              "      <td>0.880600</td>\n",
              "      <td>0.880740</td>\n",
              "      <td>0.880631</td>\n",
              "      <td>0.880594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.343500</td>\n",
              "      <td>0.336805</td>\n",
              "      <td>0.887800</td>\n",
              "      <td>0.888303</td>\n",
              "      <td>0.887858</td>\n",
              "      <td>0.887773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.320700</td>\n",
              "      <td>0.350035</td>\n",
              "      <td>0.874400</td>\n",
              "      <td>0.881850</td>\n",
              "      <td>0.874624</td>\n",
              "      <td>0.873830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.279800</td>\n",
              "      <td>0.295163</td>\n",
              "      <td>0.889600</td>\n",
              "      <td>0.889694</td>\n",
              "      <td>0.889626</td>\n",
              "      <td>0.889597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.355600</td>\n",
              "      <td>0.340158</td>\n",
              "      <td>0.881600</td>\n",
              "      <td>0.883939</td>\n",
              "      <td>0.881726</td>\n",
              "      <td>0.881443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.320200</td>\n",
              "      <td>0.341940</td>\n",
              "      <td>0.876000</td>\n",
              "      <td>0.881746</td>\n",
              "      <td>0.875804</td>\n",
              "      <td>0.875490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.333600</td>\n",
              "      <td>0.454839</td>\n",
              "      <td>0.845200</td>\n",
              "      <td>0.863744</td>\n",
              "      <td>0.844839</td>\n",
              "      <td>0.843103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.418000</td>\n",
              "      <td>0.949091</td>\n",
              "      <td>0.498400</td>\n",
              "      <td>0.249200</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.332621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.401700</td>\n",
              "      <td>0.366872</td>\n",
              "      <td>0.873000</td>\n",
              "      <td>0.873913</td>\n",
              "      <td>0.872921</td>\n",
              "      <td>0.872905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.446300</td>\n",
              "      <td>0.563584</td>\n",
              "      <td>0.687800</td>\n",
              "      <td>0.797078</td>\n",
              "      <td>0.686827</td>\n",
              "      <td>0.655500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.381700</td>\n",
              "      <td>0.390628</td>\n",
              "      <td>0.872600</td>\n",
              "      <td>0.875238</td>\n",
              "      <td>0.872466</td>\n",
              "      <td>0.872346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.282700</td>\n",
              "      <td>0.340940</td>\n",
              "      <td>0.878800</td>\n",
              "      <td>0.878863</td>\n",
              "      <td>0.878780</td>\n",
              "      <td>0.878790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.381600</td>\n",
              "      <td>0.389721</td>\n",
              "      <td>0.874400</td>\n",
              "      <td>0.877393</td>\n",
              "      <td>0.874258</td>\n",
              "      <td>0.874120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.399000</td>\n",
              "      <td>0.352053</td>\n",
              "      <td>0.878200</td>\n",
              "      <td>0.879135</td>\n",
              "      <td>0.878280</td>\n",
              "      <td>0.878140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.368400</td>\n",
              "      <td>0.437402</td>\n",
              "      <td>0.836400</td>\n",
              "      <td>0.856267</td>\n",
              "      <td>0.836022</td>\n",
              "      <td>0.833977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.382700</td>\n",
              "      <td>0.329521</td>\n",
              "      <td>0.873000</td>\n",
              "      <td>0.873625</td>\n",
              "      <td>0.873066</td>\n",
              "      <td>0.872960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>0.466099</td>\n",
              "      <td>0.760600</td>\n",
              "      <td>0.823846</td>\n",
              "      <td>0.759892</td>\n",
              "      <td>0.747977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.366300</td>\n",
              "      <td>0.372380</td>\n",
              "      <td>0.873000</td>\n",
              "      <td>0.874779</td>\n",
              "      <td>0.873111</td>\n",
              "      <td>0.872871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.367900</td>\n",
              "      <td>0.449291</td>\n",
              "      <td>0.821400</td>\n",
              "      <td>0.851474</td>\n",
              "      <td>0.820932</td>\n",
              "      <td>0.817345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.359400</td>\n",
              "      <td>0.686469</td>\n",
              "      <td>0.668800</td>\n",
              "      <td>0.795014</td>\n",
              "      <td>0.667751</td>\n",
              "      <td>0.628325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.384400</td>\n",
              "      <td>0.870448</td>\n",
              "      <td>0.719400</td>\n",
              "      <td>0.807191</td>\n",
              "      <td>0.718543</td>\n",
              "      <td>0.697301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.378300</td>\n",
              "      <td>0.526397</td>\n",
              "      <td>0.774800</td>\n",
              "      <td>0.822303</td>\n",
              "      <td>0.774185</td>\n",
              "      <td>0.765917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.360200</td>\n",
              "      <td>0.586012</td>\n",
              "      <td>0.830000</td>\n",
              "      <td>0.845149</td>\n",
              "      <td>0.830335</td>\n",
              "      <td>0.828213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.351700</td>\n",
              "      <td>0.413515</td>\n",
              "      <td>0.855800</td>\n",
              "      <td>0.858820</td>\n",
              "      <td>0.855653</td>\n",
              "      <td>0.855459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.295300</td>\n",
              "      <td>0.469729</td>\n",
              "      <td>0.845600</td>\n",
              "      <td>0.860266</td>\n",
              "      <td>0.845277</td>\n",
              "      <td>0.843925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.357400</td>\n",
              "      <td>0.359051</td>\n",
              "      <td>0.869000</td>\n",
              "      <td>0.869411</td>\n",
              "      <td>0.868947</td>\n",
              "      <td>0.868951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.409500</td>\n",
              "      <td>0.469990</td>\n",
              "      <td>0.851600</td>\n",
              "      <td>0.858893</td>\n",
              "      <td>0.851372</td>\n",
              "      <td>0.850783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.408100</td>\n",
              "      <td>0.356802</td>\n",
              "      <td>0.835200</td>\n",
              "      <td>0.846308</td>\n",
              "      <td>0.835486</td>\n",
              "      <td>0.833949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>0.384400</td>\n",
              "      <td>0.420341</td>\n",
              "      <td>0.836800</td>\n",
              "      <td>0.848936</td>\n",
              "      <td>0.836501</td>\n",
              "      <td>0.835282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.422300</td>\n",
              "      <td>0.443679</td>\n",
              "      <td>0.837600</td>\n",
              "      <td>0.848505</td>\n",
              "      <td>0.837317</td>\n",
              "      <td>0.836238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>0.448200</td>\n",
              "      <td>0.412646</td>\n",
              "      <td>0.850400</td>\n",
              "      <td>0.856474</td>\n",
              "      <td>0.850191</td>\n",
              "      <td>0.849705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.349800</td>\n",
              "      <td>0.359001</td>\n",
              "      <td>0.861400</td>\n",
              "      <td>0.869295</td>\n",
              "      <td>0.861166</td>\n",
              "      <td>0.860599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>0.279900</td>\n",
              "      <td>0.356195</td>\n",
              "      <td>0.865200</td>\n",
              "      <td>0.872024</td>\n",
              "      <td>0.864984</td>\n",
              "      <td>0.864529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.303400</td>\n",
              "      <td>0.339261</td>\n",
              "      <td>0.872800</td>\n",
              "      <td>0.874243</td>\n",
              "      <td>0.872900</td>\n",
              "      <td>0.872697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>0.315200</td>\n",
              "      <td>0.318961</td>\n",
              "      <td>0.876800</td>\n",
              "      <td>0.876811</td>\n",
              "      <td>0.876791</td>\n",
              "      <td>0.876796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.290400</td>\n",
              "      <td>0.335428</td>\n",
              "      <td>0.871800</td>\n",
              "      <td>0.872578</td>\n",
              "      <td>0.871874</td>\n",
              "      <td>0.871748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.328200</td>\n",
              "      <td>0.342026</td>\n",
              "      <td>0.874000</td>\n",
              "      <td>0.874931</td>\n",
              "      <td>0.873921</td>\n",
              "      <td>0.873904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.301700</td>\n",
              "      <td>0.343978</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.875526</td>\n",
              "      <td>0.874940</td>\n",
              "      <td>0.874943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>0.341000</td>\n",
              "      <td>0.358359</td>\n",
              "      <td>0.873800</td>\n",
              "      <td>0.873927</td>\n",
              "      <td>0.873830</td>\n",
              "      <td>0.873795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.279300</td>\n",
              "      <td>0.350443</td>\n",
              "      <td>0.872600</td>\n",
              "      <td>0.872611</td>\n",
              "      <td>0.872610</td>\n",
              "      <td>0.872600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>0.270600</td>\n",
              "      <td>0.324213</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.881447</td>\n",
              "      <td>0.880946</td>\n",
              "      <td>0.880953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.252100</td>\n",
              "      <td>0.339765</td>\n",
              "      <td>0.875200</td>\n",
              "      <td>0.879785</td>\n",
              "      <td>0.875025</td>\n",
              "      <td>0.874785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>0.235700</td>\n",
              "      <td>0.374933</td>\n",
              "      <td>0.874800</td>\n",
              "      <td>0.878148</td>\n",
              "      <td>0.874951</td>\n",
              "      <td>0.874553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.329800</td>\n",
              "      <td>0.316495</td>\n",
              "      <td>0.879400</td>\n",
              "      <td>0.881459</td>\n",
              "      <td>0.879283</td>\n",
              "      <td>0.879212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>0.275800</td>\n",
              "      <td>0.325449</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.881250</td>\n",
              "      <td>0.881042</td>\n",
              "      <td>0.880988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.260300</td>\n",
              "      <td>0.320771</td>\n",
              "      <td>0.880000</td>\n",
              "      <td>0.880985</td>\n",
              "      <td>0.880082</td>\n",
              "      <td>0.879938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.292500</td>\n",
              "      <td>0.333683</td>\n",
              "      <td>0.882200</td>\n",
              "      <td>0.882592</td>\n",
              "      <td>0.882252</td>\n",
              "      <td>0.882179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.273400</td>\n",
              "      <td>0.300440</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.881205</td>\n",
              "      <td>0.881038</td>\n",
              "      <td>0.880990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>0.263100</td>\n",
              "      <td>0.348171</td>\n",
              "      <td>0.874400</td>\n",
              "      <td>0.878343</td>\n",
              "      <td>0.874564</td>\n",
              "      <td>0.874105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.287700</td>\n",
              "      <td>0.315841</td>\n",
              "      <td>0.884600</td>\n",
              "      <td>0.885811</td>\n",
              "      <td>0.884690</td>\n",
              "      <td>0.884526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4450</td>\n",
              "      <td>0.267900</td>\n",
              "      <td>0.320113</td>\n",
              "      <td>0.883000</td>\n",
              "      <td>0.883744</td>\n",
              "      <td>0.883071</td>\n",
              "      <td>0.882956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.276100</td>\n",
              "      <td>0.313159</td>\n",
              "      <td>0.883600</td>\n",
              "      <td>0.883658</td>\n",
              "      <td>0.883621</td>\n",
              "      <td>0.883598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4550</td>\n",
              "      <td>0.261200</td>\n",
              "      <td>0.317065</td>\n",
              "      <td>0.882600</td>\n",
              "      <td>0.884456</td>\n",
              "      <td>0.882489</td>\n",
              "      <td>0.882436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.278000</td>\n",
              "      <td>0.318511</td>\n",
              "      <td>0.882800</td>\n",
              "      <td>0.884948</td>\n",
              "      <td>0.882681</td>\n",
              "      <td>0.882612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>0.293100</td>\n",
              "      <td>0.320999</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.886291</td>\n",
              "      <td>0.885957</td>\n",
              "      <td>0.885969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.274300</td>\n",
              "      <td>0.312472</td>\n",
              "      <td>0.884800</td>\n",
              "      <td>0.884799</td>\n",
              "      <td>0.884803</td>\n",
              "      <td>0.884800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>0.301000</td>\n",
              "      <td>0.311812</td>\n",
              "      <td>0.886200</td>\n",
              "      <td>0.886261</td>\n",
              "      <td>0.886180</td>\n",
              "      <td>0.886191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.275600</td>\n",
              "      <td>0.304769</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>0.882570</td>\n",
              "      <td>0.881103</td>\n",
              "      <td>0.880897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4850</td>\n",
              "      <td>0.252600</td>\n",
              "      <td>0.306757</td>\n",
              "      <td>0.885200</td>\n",
              "      <td>0.885734</td>\n",
              "      <td>0.885260</td>\n",
              "      <td>0.885171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.279200</td>\n",
              "      <td>0.308880</td>\n",
              "      <td>0.885200</td>\n",
              "      <td>0.885605</td>\n",
              "      <td>0.885253</td>\n",
              "      <td>0.885179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.305876</td>\n",
              "      <td>0.888200</td>\n",
              "      <td>0.888299</td>\n",
              "      <td>0.888226</td>\n",
              "      <td>0.888197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.247900</td>\n",
              "      <td>0.304250</td>\n",
              "      <td>0.888600</td>\n",
              "      <td>0.888839</td>\n",
              "      <td>0.888640</td>\n",
              "      <td>0.888589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5050</td>\n",
              "      <td>0.254600</td>\n",
              "      <td>0.319854</td>\n",
              "      <td>0.886000</td>\n",
              "      <td>0.887064</td>\n",
              "      <td>0.886085</td>\n",
              "      <td>0.885936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.317100</td>\n",
              "      <td>0.304759</td>\n",
              "      <td>0.886600</td>\n",
              "      <td>0.887524</td>\n",
              "      <td>0.886679</td>\n",
              "      <td>0.886546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5150</td>\n",
              "      <td>0.250100</td>\n",
              "      <td>0.305717</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>0.888216</td>\n",
              "      <td>0.888039</td>\n",
              "      <td>0.887991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.253000</td>\n",
              "      <td>0.307376</td>\n",
              "      <td>0.888800</td>\n",
              "      <td>0.889096</td>\n",
              "      <td>0.888845</td>\n",
              "      <td>0.888786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5250</td>\n",
              "      <td>0.239600</td>\n",
              "      <td>0.306787</td>\n",
              "      <td>0.887800</td>\n",
              "      <td>0.888238</td>\n",
              "      <td>0.887855</td>\n",
              "      <td>0.887777</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Checkpoint destination directory ./checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5250, training_loss=0.3536335743495396, metrics={'train_runtime': 3575.3818, 'train_samples_per_second': 29.367, 'train_steps_per_second': 1.468, 'total_flos': 2.76266608128e+16, 'train_loss': 0.3536335743495396, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Record test results\n",
        "results3 = trainer3.predict(test_dataset)\n",
        "results3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Z1zONFOsyawP",
        "outputId": "4881afe4-c260-44a6-e30c-502d8a9464e4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-1.6181641,  1.8271484],\n",
              "       [-1.3271484,  1.5234375],\n",
              "       [-1.5976562,  1.8076172],\n",
              "       ...,\n",
              "       [-1.2226562,  1.3203125],\n",
              "       [ 1.1445312, -1.7802734],\n",
              "       [-1.5087891,  1.7255859]], dtype=float32), label_ids=array([1, 1, 1, ..., 0, 0, 1]), metrics={'test_loss': 0.3318052589893341, 'test_accuracy': 0.8768, 'test_precision': 0.8772685254271748, 'test_recall': 0.876705684737767, 'test_f1': 0.8767392521730409, 'test_runtime': 38.7781, 'test_samples_per_second': 257.877, 'test_steps_per_second': 8.613})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}